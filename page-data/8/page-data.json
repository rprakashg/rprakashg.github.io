{
    "componentChunkName": "component---src-templates-post-list-js",
    "path": "/8",
    "result": {"data":{"site":{"siteMetadata":{"title":"rprakashg.github.io","author":"RAM GOPINATHAN"}},"allMarkdownRemark":{"totalCount":25,"edges":[{"node":{"excerpt":"Overview Kinesis firehose is a managed service within AWS that can be used to capture streaming data and load it into Kinesis Analytics, S3, Amazon Redshift or Amazon ElasticSearch services. I've…","html":"<h1>Overview</h1>\n<p>Kinesis firehose is a managed service within AWS that can be used to capture streaming data and load it into Kinesis Analytics, S3, Amazon Redshift or Amazon ElasticSearch services. I've published a cloudformation template that automates provisioning of all required components for Kinesis Firehose with AWS S3 delivery on your AWS account. You can find more info about the cloudformation template <a href=\"https://github.com/rprakashg/cf-templates/tree/master/firehose-with-s3-destination\">here</a></p>\n<p>One thing I should point out is one of the thing template does is it provisions a KMS key which is used to encrypt/decrypt data ingested at REST. If you want to give others access to read this data from S3 in addition to granting read access to S3 bucket you'll also need to grant DECRYPT permission in your KMS key policy. I din't add this into the template because I don't think it would be good idea to allow decrypt permission to all by default. So depending on your use case if you need to allow consuming applications read access to this data you will need to modify the KMS key policy and grant 'Decrypt' permission as shown below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">{\n    \"Sid\": \"Allow decrypt\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        //include service or principal depending on your usecase\n    },\n    \"Action\": \"kms:Decrypt\",\n    \"Resource\": \"*\"\n}</code></pre></div>\n<p>Hope that helps.</p>\n<p>Cheers,</p>\n<p>Ram</p>","id":"1703a8ca-a1e9-53c2-a5e0-51fe01530b8f","frontmatter":{"title":"Cloudformation template for provisioning firehose with S3 delivery","date":"June 27, 2017","tags":["cloud","cloudformation","aws","kinesis","firehose"],"author":"Ram Gopinathan"},"fields":{"slug":"/cf-template-for-kinesis-firehose/"}}},{"node":{"excerpt":"At T-Mobile we use jFrog artifactory as a centralized repository for storing artifacts. It is one of the key tools in our DevOps toolset and is integrated into our CI/CD processes. Artifactory has…","html":"<p>At T-Mobile we use jFrog <a href=\"https://www.jfrog.com/artifactory/\">artifactory</a> as a centralized repository for storing artifacts. It is one of the key tools in our DevOps toolset and is integrated into our CI/CD processes. Artifactory has support for storing docker images in repositories, essentially its like running private docker registries.\nArtifactory lets you create multiple docker repositories which is pretty nice for a large company like T-Mobile where there are numerous groups developing dockerized microservices, each group can have thier own repository (registry in docker terminology) with right level of access control that governs who can push images into it.</p>\n<p>If you are using Artifactory for hosting private Docker registries in enterprise, below are few things to keep in mind. Later I will cover how we have addressed these.</p>\n<ul>\n<li>\n<p>Discovering images that are stored in multiple repositories</p>\n<p>With flexibility of running multiple docker registries, discovering images becomes bit of a challenge, artifactory web portal has some nice search functionality built in which helps, but if you are in docker cli you have to know the name of the repository before you can search or pull the image down</p>\n</li>\n<li>\n<p>Governing Use of Docker Images from Docker Hub</p>\n<p>Docker hub is public registry where thousands of open source developers and ISVs are publishing docker images for thier products. In the enterprise it becomes critical to ensure the use of appropriate images for production deployment. You don't want a group by mistake use an image that has security vulnerabilities or is not certified by docker to run well on docker platforms. The new docker store addresses some of these challenges enterprises face but governing use of docker images for production still is a critical things every enterprise needs to ensure.</p>\n</li>\n<li>\n<p>Promoting docker images to release registry</p>\n<p>One of the policies we have is any docker image deployed to our production Mesos stack has to come from release repository in artifactory. Only the service account used by Jenkins job has rights to push to release registry. For those who might be new to Artifactory, when you create docker repository in Artifactory, artifactory creates \"<em>-snapshot-local\" and \"</em>-release-local\" repositories. Snapshot is where images that are still in development stored, before a docker image is deployed to production they get promoted to release repository. Another benefit with this model is that you can have seperate retention policies for images stored in these repos, for ex. We have a default 15 day retention policy for anything in Snapshot, you can request to increase this. Obviously images in release repo has unlimited retention</p>\n</li>\n</ul>\n<h2>How did we address the above three things</h2>\n<p>For the first bullet point, we use a virtual docker repository that aggregates images from all docker repositories in artifactory. Few benefits of doing this is that</p>\n<ol>\n<li>\n<p>It Provides a single endpoint for developers to resolve images from docker registries in artifactory. This is huge in the enterprise, think about in the past if you needed to reuse something another group had built, I can tell you it almost never happens, there are many reasons for this which I wont get in to but in summary lots of duplication of effort is what ends up happening. Containerization movement within our company and use of Docker and Artifactory is really changing how teams work, there is less duplication, developers can build/push images into docker registries in artifactory and other developers inside and outside of their respective groups can discover and use these images avoiding duplication of efforts etc.</p>\n</li>\n<li>\n<p>The other key benefit is IT operators can manage these docker registries such as moving/consolidating images or modify include and exclude patterns to enfore security policies without worrying about impacting developers.</p>\n</li>\n</ol>\n<p>For the second bullet point we are using remote repository which is seeded with approved images for production use from docker hub. Virtual repository also aggregates images from remote repository freeing developers to ever needing to pull any image from docker hub.</p>\n<p>For the third bullet point we use docker tagging in Jenkins job to promote images that are ready for production deployment from snapshot to release.</p>\n<p>Jenkins job first we pull the image from snapshot registry as shown below...</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">docker pull {artifactory host}/{docker snapshot registry}/{image name}:{tag} \n</code></pre></div>\n<p>Next we use docker tagging to include release registry as shown below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">docker tag {artifactory host}/{docker snapshot registry}/{image name}:{tag} {artifactory host}/{docker release registry}/{image name}:{tag}</code></pre></div>\n<p>Lastly we use docker push to promote the image to release registry</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">docker push artifactory host}/{docker release registry}/{image name}:{tag}</code></pre></div>\n<p>Below is a diagram that shows how we use artifactory for hosting private docker registries</p>\n<p><img src=\"http://rprakashg.github.io/images/artifactory.jpg\" alt=\"artifactory structure\"></p>","id":"b4c5718e-8488-5ff5-b3c9-6c39b099665c","frontmatter":{"title":"Some thoughts on running docker registries in jFrog artifactory","date":"May 07, 2017","tags":["docker","artifactory","registry","discovery"],"author":"Ram Gopinathan"},"fields":{"slug":"/docker-registries-with-artifactory/"}}},{"node":{"excerpt":"Why changing the blogging platform There is two main reasons why I switched hugo for blogging. Speed (No more dynamic rendering of pages based on content stored in databases) Cost Hugo gives me…","html":"<h2>Why changing the blogging platform</h2>\n<p>There is two main reasons why I switched hugo for blogging.</p>\n<ul>\n<li>Speed (No more dynamic rendering of pages based on content stored in databases)</li>\n<li>Cost</li>\n</ul>\n<p>Hugo gives me freedom from needing any runtimes or databases which equates to speed since the entire site is just plain old HTML generated out of markdown files. Additionally my current wordpress blog hosted on Azure with ClearDB is running out of space available for free tier and is requiring me to upgrade to paid membership.</p>\n<p>I wanted to author blog posts using markdown format, VisualStudio Code provides great support for markdown authoring. Posts authored in markdown format are stored in a github <a href=\"http://github.com/rprakashg/blog\">repository</a>. Additionally I needed a continous publishing process that builds the site using hugo and publishes generated content when ever changes are committed to github repository.</p>\n<h2>Hosting platform</h2>\n<p>For hosting I decided to use AWS S3 with Cloudfront which gives me low cost storage and global content delivery.</p>\n<h2>Provisioning AWS components</h2>\n<p>Creating a new site and configuring it for blog was pretty easy and straightforward, I'm not going to go into details as there is plenty of articles and even documentation available at <a href=\"http://gohugo.io\">gohugo.io</a> site how ever I ran into some challenges with S3 + Cloudfront hosting. I came across a <a href=\"https://s3-us-west-2.amazonaws.com/cloudformation-templates-us-west-2/S3_Website_With_CloudFront_Distribution.template\">sample template</a> for S3 hosting but quickly ran into few limitations</p>\n<ol>\n<li>After I setup continuous publishing from Travis-CI I was getting access denied when accessing items in the bucket even though the bucket had public read enabled, this is primarily due to the fact that there is no concept of inheriting permissions in S3. I ended up having to create a bucket policy that granted read access to everything in the bucket to everyone</li>\n<li>Since one of my goal with this move was to setup continous publishing from Travis-CI when ever changes are committed to the github repository, it required me to create an AWS IAM user with full rights to S3 bucket hosting the content, additionally I needed to invalidate the cloudfront distribution when ever changes are published to the S3 bucket so user's can see fresh content, this required few cloudfront specific permissions granted to the user.</li>\n</ol>\n<p>Because of the reasons mentioned above and a few other flexibilities I was looking for, I ended up creating a custom cloudformation template. You can find the template <a href=\"http://github.com/rprakashg/cf-templates\">here</a>. If you want to leverage hugo for hosting your blog or other types of sites along with S3 + Cloudfront for hosting and content delivery, you will find this template very useful as it will get you almost 98% of the way. Once stack is created using the template you can simply copy the nameservers from the hosted zone created and update your domain settings in godaddy or what ever hosting provider you use.</p>\n<h2>Creating the stack</h2>\n<p>After you clone the <a href=\"http://github.com/rprakashg/cf-templates\">repo</a>, simply change the default template parameter values in template.parameters.json file to match your needs. Script uses AWS cli create-stack command to create the stack. If you are using OSX you can run command below to create the stack</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">./create-stack.sh</code></pre></div>\n<p>Once the stack is created successfully simply login to your AWS console and navigate to Route 53 hosted zone that was created by the cloudformation template and copy the name servers list and update your domain settings and replace nameservers with the one's you copied from the hosted zone recordset</p>\n<h2>Deleting stack</h2>\n<p>I've also included a simple script to delete the stack, script will grab the stack name from template.parameters.json file and deletes the stack using AWS cli delete-stack command. Run command below to delete stack</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">./delete-stack.sh</code></pre></div>\n<h2>Continuous publishing</h2>\n<p>As mentioned earlier one of my goal was to continuously publish to S3 bucket when new content or changes to existing content are committed to github repository. I'm using Travis-CI for this. If you are not familiar with Travis head over to <a href=\"http://travis-ci.org\">Travis-CI.org</a> for more info.</p>\n<p>See my <a href=\"http://github.com/rprakashg/blog/blob/master/.travis.yml\">.travis.yml</a> for reference.</p>\n<p>As you can see from my travis file I install hugo, then generate site content by running hugo command in script section. For deploying to S3 I'm using the S3 deployment support available in Travis-CI. For more information please <a href=\"https://docs.travis-ci.com/user/deployment/s3/\">see</a>. I've also added AWS Credential ID and Secret Key for the IAM build user that gets created as a part of creating the stack using the cloudformation template. AWS Credentials are stored encrypted. For more information on adding encrypted environment variables <a href=\"https://docs.travis-ci.com/user/environment-variables/#Defining-encrypted-variables-in-.travis.yml\">see</a>. Finally after successfully publishing content we invalidate cloudfront distribution by running a script. Download the cloudfront invalidation script <a href=\"https://github.com/rprakashg/blog/blob/master/cdn-invalidate.sh\">here</a>. Big thanks to <a href=\"https://www.whaletech.co/about/\">Ben Whaley</a> for the cloudfront invalidation script.</p>","id":"862f5aa1-511e-5b76-a393-1622e9f42224","frontmatter":{"title":"New blog","date":"April 24, 2017","tags":["aws","s3","cloudfront","hugo","website","hosting"],"author":"Ram Gopinathan"},"fields":{"slug":"/new-blog/"}}}]}},"pageContext":{"limit":3,"skip":21,"numPages":10,"currentPage":8}},
    "staticQueryHashes": ["1611934721","2366241629"]}