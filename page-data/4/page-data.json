{"componentChunkName":"component---src-templates-post-list-js","path":"/4/","result":{"data":{"site":{"siteMetadata":{"title":"rprakashg.github.io","author":"RAM GOPINATHAN"}},"allMarkdownRemark":{"totalCount":40,"edges":[{"node":{"excerpt":"Observability service is one of the capabilities provided by Red Hat Advanced Cluster Management for Kubernetes that helps platform engineering teams monitor and observe the health and capacity of…","html":"<p>Observability service is one of the capabilities provided by Red Hat Advanced Cluster Management for Kubernetes that helps platform engineering teams monitor and observe the health and capacity of clusters across entire kubernetes fleet.</p>\n<h2>Purpose</h2>\n<p>Kubernetes cluster sprawl is real, we see customers deploying clusters on-premises, multi cloud and even on edge locations to run workloads. With this comes a set of management challenges for platform engineering teams. Being able to monitor the health and utilization of clusters across the entire k8s fleet is one of the challenges platform engineering teams have to solve. Typically this involves deploying a central monitoring infrastructure, select a monitoring solution (Prometheus is most common) and ensure provisioned clusters are configured to send telemetry data to this central infrastructure so they can monitor the health and utilization of clusters across the entire k8s fleet in one place. Observability service in RHACM solves this problem so our customers don't have to. We also understand that not all customers use Openshift for running workloads also some have multiple k8s distributions and as long as those clusters are imported into hub, this feature should work well as long as they are complaint with upstream k8s</p>\n<h2>Summary of what's configured when you enable observability service</h2>\n<p>When you enable the observability service, a Prometheus/Thanos stack is deployed on the hub cluster and data collector components are deployed on every managed cluster for collecting and aggregating telemetry information.\nAll required k8s resources are provisioned for persistent storage for various components of thanos. Object store is provisioned for metrics storage.\nAdditionally a Grafana instance is provisioned with a set of useful dashboards that platform teams can use to monitor the health and utilization. A Grafana dashboard loader sidecar is also deployed with Grafana instance to load dashboards into Grafana. Platform teams can create additional dashboards and add them to a configmap and these dashboards will be automatically loaded into Grafana instance by the Grafana dashboard loader sidecar. Prometheus alertmanager can also be configured to send monitoring alerts to slack, pagerduty etc.</p>\n<p>Diagram below shows various components configured when observability service is configured on hub cluster</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/06bb1f4cc0d3a5de38295ec634847189/ab75a/acm.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe1NCg//xAAWEAEBAQAAAAAAAAAAAAAAAAABEAD/2gAIAQEAAQUCxG//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAEAAQUAAAAAAAAAAAAAAAABABEgUWGR/9oACAEBAAE/IUcxL2U3Y//aAAwDAQACAAMAAAAQkM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhMRBRcf/aAAgBAQABPxB4VZG61cQBAyMPUEXk+zQho4//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"observability\"\n        title=\"\"\n        src=\"/static/06bb1f4cc0d3a5de38295ec634847189/4b190/acm.jpg\"\n        srcset=\"/static/06bb1f4cc0d3a5de38295ec634847189/e07e9/acm.jpg 200w,\n/static/06bb1f4cc0d3a5de38295ec634847189/066f9/acm.jpg 400w,\n/static/06bb1f4cc0d3a5de38295ec634847189/4b190/acm.jpg 800w,\n/static/06bb1f4cc0d3a5de38295ec634847189/e5166/acm.jpg 1200w,\n/static/06bb1f4cc0d3a5de38295ec634847189/ab75a/acm.jpg 1428w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>Automating enabling the observability service</h2>\n<p>Observability service is not enabled by default, steps to enable this feature is documented very well in RHACM product documentation <a href=\"https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.10/html/observability/enabling-observability-service\">here</a>. I recommend reading that. I wrote an ansible playbook that automates this. You can check out the ansible playbook <a href=\"https://github.com/rprakashg-redhat/rhacm-demos/blob/main/observability/configure-multiclusterobservability.yaml\">here</a></p>\n<p><a href=\"https://asciinema.org/a/yfJKLHpEXkar3lzNibOgNSgW0\"><img src=\"https://asciinema.org/a/yfJKLHpEXkar3lzNibOgNSgW0.svg\" alt=\"asciicast\"></a></p>\n<p>We should now be able to log back into hub cluster and see link to Grafana instance in the clusters view under infrastructure section as shown in screen capture below</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b3624ad3a011e24f69d5e023a772fabf/c2d13/observability-acm.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 34%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABRklEQVR42nWQS0/DMBCE80tI0zbpQ4gTAipagbhxK1UP/GSu3KgUCZqeaMmLPGzHdoZ1QipVgkifZr0b70xieYMhzmwH9uUjejdL9GdPcAj7egmHzqPbFUbzlvFihQnhUc/MDO58DW+xbnRy/wzrajbH4u4Br5sPJEwjLhXiQkKoGnmFI5kg/cXUKW9JWI2vokaY0T1qWN74HOPpBYIggHmUYCjKEi9+irQUkBWHVrRRK6DWp2jd9LWUhECVf8Pq9T0M3QnetzvQGJwxMFHhM8poMUNJpFmBknGqSwghWrgAFxycDBvlvOlbNi10vSm2wa5NSG6SnLUiNc5US6mamtMSc+6Q9I6gZGbWGVm242JACX3fp8+rkGWUrCiaNH9hZh15nh8x94xaZpn5h2+bTdOIoghhGOJwOPyLmcdxfEKSJNjv9/gBX2nUotDM8toAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"observability\"\n        title=\"\"\n        src=\"/static/b3624ad3a011e24f69d5e023a772fabf/5a190/observability-acm.png\"\n        srcset=\"/static/b3624ad3a011e24f69d5e023a772fabf/772e8/observability-acm.png 200w,\n/static/b3624ad3a011e24f69d5e023a772fabf/e17e5/observability-acm.png 400w,\n/static/b3624ad3a011e24f69d5e023a772fabf/5a190/observability-acm.png 800w,\n/static/b3624ad3a011e24f69d5e023a772fabf/c1b63/observability-acm.png 1200w,\n/static/b3624ad3a011e24f69d5e023a772fabf/29007/observability-acm.png 1600w,\n/static/b3624ad3a011e24f69d5e023a772fabf/c2d13/observability-acm.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Clicking on the link will bring up the Grafana instance that is configured with set of dashboards. Grafana instance is configured to be able to login with the same credentials you use to login to Openshift/Hub cluster. To illustrate that this works with other k8s distributions I've provisioned an EKS cluster called <code class=\"language-text\">toolscluster</code> in <code class=\"language-text\">us-east-2</code> region and have imported that cluster into ACM hub</p>\n<p>Including screen captures for few of the observability dashboards that we ship</p>\n<p>Dashboard below shows compute resources by cluster. You can select cluster from the dropdown selection at the top and report will be refreshed</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/31ae9d91b35c22c6d935a35a09f80681/c2d13/observability-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 34%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABJElEQVR42l1R2XLCMAzMJxAghy/5SJyUo6GUdvj/L9vKCmUYHnZ2rVtyRWGC0p4R0HQWm22H+h27XnhTt0/bpu6w3Sm0nNMpwr41UqdyfsBpuaLpreDz6wc+ZWwbhXE+In+cRbsw4Hq7o9Mkccv1F3GcUe97DNMRE8d12qHqDUG7iFY5gXZBHI2yUNajNwwbRFuf0OoSZyWn+NpH3D8qF0dY7l541Yl5YB6hfYRhFL9hu6LA7yTQFJ9asaaUsCwzF+REw92c54Ihg+IEYjaUZAJlo0xTeEV40+tbM1NMqA7nG2I+cFLilTJ3HnnFdR3HxTUN4nNx5kkntoeHbZAYw7qg6BJXXb7vmE8XWXFdO69r8C0pjS86yymKtvyRBcVXzvGq/wD2Rbxx4FUaygAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"observability1\"\n        title=\"\"\n        src=\"/static/31ae9d91b35c22c6d935a35a09f80681/5a190/observability-1.png\"\n        srcset=\"/static/31ae9d91b35c22c6d935a35a09f80681/772e8/observability-1.png 200w,\n/static/31ae9d91b35c22c6d935a35a09f80681/e17e5/observability-1.png 400w,\n/static/31ae9d91b35c22c6d935a35a09f80681/5a190/observability-1.png 800w,\n/static/31ae9d91b35c22c6d935a35a09f80681/c1b63/observability-1.png 1200w,\n/static/31ae9d91b35c22c6d935a35a09f80681/29007/observability-1.png 1600w,\n/static/31ae9d91b35c22c6d935a35a09f80681/c2d13/observability-1.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Dashboard below shows compute resources by node</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c48383a32639c09bf2231d9e948ef919/c2d13/compute-resources-node.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABiElEQVR42k1Ry3LTQBDUL4CNLUsraXf1flhWcA4UBy7JgRMUVdz4Z4yTcuKCgiiQCvxF07MWVA5d3dMzszs76x2/PeBw/Inr23tc3d5hdzNid/0dnw/kwx329L/c3GPv9Igr+uLtHLPv+As/xkeM42/sv/6B17QDyrJF3fQoyg5RbBCqBEppqMggmqAc9Mn/B9ZqkyFIDJbs0TaHN/cVni8CzJchZoRowewJL1YR5n7o9FNI/tkLHxfDa1wOb5DbBl7V9qg7TslJq3aDslmjkpgT1+sz8mbye9b0p5zEk5bch7fv8On9R7w8O4eX85lp3kCnlWOT1bCOG8Yt/RJ5tUZWdDBTjaVvMtE10qLFkuuYBTECstf2W94yuERkSizDBD73ESY5Voq7CWPqzMWnnMYqsvBFE6soRRBbHsadJim8bjh3EyidOWPBm6RRYmmWWHQ45fxIDtSuRiCHCS8C0Zxw2L5Ct9ki4dOStOCPWTYbxLYgW9cc8/dikzutdPo/J7B8tniihf8CMwsKOXc8ftUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"resourcesbynode\"\n        title=\"\"\n        src=\"/static/c48383a32639c09bf2231d9e948ef919/5a190/compute-resources-node.png\"\n        srcset=\"/static/c48383a32639c09bf2231d9e948ef919/772e8/compute-resources-node.png 200w,\n/static/c48383a32639c09bf2231d9e948ef919/e17e5/compute-resources-node.png 400w,\n/static/c48383a32639c09bf2231d9e948ef919/5a190/compute-resources-node.png 800w,\n/static/c48383a32639c09bf2231d9e948ef919/c1b63/compute-resources-node.png 1200w,\n/static/c48383a32639c09bf2231d9e948ef919/29007/compute-resources-node.png 1600w,\n/static/c48383a32639c09bf2231d9e948ef919/c2d13/compute-resources-node.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>and Dashboard below shows a clusters overview.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a4772fea9c6f9ffbfc41104e7d2b91c9/c2d13/clusters-overview.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 34%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABQElEQVR42m1Qy07DQAzMlQOPNkr2vZtN0qSUIg6tEGoVpLYc4QoS8Bf9fw1elxaQOFhej71jz2QhTmFdg9GlwPlZjqsLgXEuMRqL/2NUnt7jXKAUhsKiVA4uNMhCPYU0FW6eFnh4H3D/vIaLPaSrYKoJlIswoSWsg7QJ606YpWOUjbA0pwljQls1EMajH+a4e1lgup5THehzYFLOFOr41gFCer4oLfjdMz4SoSdC7flKTY1Q97C0/YCFQyaS1BfKQ9c1fD9BM5v9mStTj67MQnM4e/m2wm6/w/CxYcna1+BekkJyYzcjwoDl6wrb/RaPnxsi7HjO81yNpDZLp5bG0QYyVlsU0nCU2nHNWbnv+gcrJM0qe8ILwpPCLLbXcGSq9S1H29+ybENWGNqesiavVPIreUsSU13RjI8T9u8YycMvXmm+1EZf0VIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"overview\"\n        title=\"\"\n        src=\"/static/a4772fea9c6f9ffbfc41104e7d2b91c9/5a190/clusters-overview.png\"\n        srcset=\"/static/a4772fea9c6f9ffbfc41104e7d2b91c9/772e8/clusters-overview.png 200w,\n/static/a4772fea9c6f9ffbfc41104e7d2b91c9/e17e5/clusters-overview.png 400w,\n/static/a4772fea9c6f9ffbfc41104e7d2b91c9/5a190/clusters-overview.png 800w,\n/static/a4772fea9c6f9ffbfc41104e7d2b91c9/c1b63/clusters-overview.png 1200w,\n/static/a4772fea9c6f9ffbfc41104e7d2b91c9/29007/clusters-overview.png 1600w,\n/static/a4772fea9c6f9ffbfc41104e7d2b91c9/c2d13/clusters-overview.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Hope this helps,</p>\n<p>As always feel free to reach out to me if you have any questions about this post or how Red Hat Advanced Cluster Management for Kubernetes can help with managing your k8s fleet</p>\n<p>Thanks,\nRam</p>","id":"d2eebb5a-3cd0-56a2-bec2-211d427dd060","frontmatter":{"title":"Using Ansible to configure observability service in Red Hat Advanced Cluster Management for Kubernetes","date":"March 22, 2024","tags":["OpenShift","Platform","Kubernetes","Management"],"author":"Ram Gopinathan"},"fields":{"slug":"/observability-service-in-rhacm/"}}},{"node":{"excerpt":"In my current role I'm always having a ton of conversations with customers regarding Kubernetes fleet management and managing clusters at scale in hybrid cloud environments. While we have a lot of pre…","html":"<p>In my current role I'm always having a ton of conversations with customers regarding Kubernetes fleet management and managing clusters at scale in hybrid cloud environments. While we have a lot of pre-built demos at <a href=\"https://demo.redhat.com\">demo.redhat.com</a> I usually end up my building a lot of it myself because I can really make something thats very specific to a particular customer I'm working and its also easier to explain and walkthrough something you built v/s what someone else built.</p>\n<p>The way I use to install and configure before was by logging into OpenShift console and following installation instructions in our product documentation for Red Hat Advanced Cluster Management for Kubernetes. This started to become so time consuming and inefficient over time and I decided to use Ansible to automate installing and configuring ACM. Ansible is great at configuring infrastructure once its provisioned.</p>\n<p>Ansible community has created a collection for kubernetes, if you are not familiar with it please check out this <a href=\"https://docs.ansible.com/ansible/latest/collections/kubernetes/core/index.html#plugins-in-kubernetes-core\">documentation</a>. I'm using the k8s module from this collection which allows me to manage k8s objects on a kubernetes cluster.</p>\n<p>You can find everything I'm covering in this post here in this <a href=\"https://github.com/rprakashg-redhat/rhacm-demos/tree/main/install\">directory</a> of Github <a href=\"https://github.com/rprakashg-redhat/rhacm-demos\">repo</a>. You are welcome to use it at your own risk.</p>\n<p>To install and configure ACM I first login to the OpenShift cluster where I want to install ACM and then run this command <em><code class=\"language-text\">ansible-playbook install/install-configure-acm-playbook.yaml</code></em>. The Ansible playbook is pretty self explanatory so I wont go into too much details but at a high level what happens is as follows</p>\n<ul>\n<li>Load static variable values defined in yaml file.</li>\n<li>Extract pull secret that was used when cluster was installed.</li>\n<li>Write the data into a dynamic vars yaml file (I use .gitignore to exclude everything in dynamic vars directory so it doesn't get accidently checked in to Github).</li>\n<li>Load the dynamic variable values.</li>\n<li>Create open cluster management namespace.</li>\n<li>Create the operator group.</li>\n<li>Install ACM operator and wait for operator to be ready.</li>\n<li>With the pull secret that was extracted and saved into dynamic variable values file create a pull secret in open cluster management namespace for multi cluster hub.</li>\n<li>Create an instance of multi cluster hub and wait for the hub to be ready</li>\n<li>Install OpenShift GitOps Operator (ArgoCD) and wait for operator to be ready</li>\n</ul>\n<p><a href=\"https://asciinema.org/a/AQBvzRfPARRyRglJ8smO1FNZk\"><img src=\"https://asciinema.org/a/AQBvzRfPARRyRglJ8smO1FNZk.svg\" alt=\"asciicast\"></a></p>\n<p>Now we can switch back to Openshift console and refresh the console to check the status of ACM install and configuration. From the screen capture below you can see we now have RHACM all installed and configured</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4ef5ca703521ae592cbc761e2f79bd62/45689/acm-install-result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.50000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABeUlEQVR42pWOT0sbQRjG91sUUWO0apSqLaH3VqmBKkhprh489I8UqYdCj4mhggdB889GsypV4sFPYQ+aQr146rGXfoBsqruZ3ZnNr7OznnrSgR/P874P88xYpycnnDYaHB5+o1YpU93e4mC3xkHtK/Vqxfgju87xvk1tp0q5WKRSKlEuFWNu54gdfd9K9PbT35fgQU8fmex7Flc2+LhWZ7Vgs5Lb1d5mNb/H53WbpU+bLLzLkf1Q4LXm1XKB+bdrvHyTZ07rwvIXrJnMHM+mZ3k+/YLzn1eEQNANNSA10azlzseaevKUycdpRlKPaDabZum5N7rV49cfh8vfDr4QCM8llD5KE8ogRsXa1aoCnQUCa2IqzdDwGInkMOcXP+JCz0MqybUrcK49fD/AdTv8vfHoCJ9OR+idj4geusXXhSIqTI1PmsKBwREumnGhlJIwDOkalPERUoUopUz+P0EQmE9YyaFRBh+miPTs7LsJWq0W7Xb7XjiOY/gHmoPC3nkd4U0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rhacm\"\n        title=\"\"\n        src=\"/static/4ef5ca703521ae592cbc761e2f79bd62/5a190/acm-install-result.png\"\n        srcset=\"/static/4ef5ca703521ae592cbc761e2f79bd62/772e8/acm-install-result.png 200w,\n/static/4ef5ca703521ae592cbc761e2f79bd62/e17e5/acm-install-result.png 400w,\n/static/4ef5ca703521ae592cbc761e2f79bd62/5a190/acm-install-result.png 800w,\n/static/4ef5ca703521ae592cbc761e2f79bd62/c1b63/acm-install-result.png 1200w,\n/static/4ef5ca703521ae592cbc761e2f79bd62/29007/acm-install-result.png 1600w,\n/static/4ef5ca703521ae592cbc761e2f79bd62/45689/acm-install-result.png 2249w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>If you have any questions please feel free to reach out.</p>\n<p>Hope that was helpful</p>\n<p>Thanks,\nRam</p>","id":"6fdcd152-8edc-549a-af62-e6dba93f28aa","frontmatter":{"title":"Using Ansible for installing and configuring Red Hat Advanced Cluster Management for Kubernetes (RHACM)","date":"March 22, 2024","tags":["OpenShift","Platform","Kubernetes","Management"],"author":"Ram Gopinathan"},"fields":{"slug":"/using-ansible-to-install-configure-rhacm/"}}},{"node":{"excerpt":"As many of you may already know Red Hat Developer hub which is based on backstage project created by Spotify hit GA. If you are not familiar with RedHat Developer hub or backstage check out this…","html":"<p>As many of you may already know Red Hat Developer hub which is based on backstage project created by Spotify hit GA. If you are not familiar with RedHat Developer hub or backstage check out this <a href=\"https://developers.redhat.com/rhdh\">article</a>. You can deploy and run developer hub on all x-K8s (AKS, EKS, GKE). In this post I will cover how to install and configure RedHat developer hub on Amazon Elastic Kubernetes Service (EKS). If you are using AKS or GKE I would think these steps would work just fine although I haven't tested in AKS or GKE. Everything I'm covering in this post can be found in this Github <a href=\"https://github.com/rprakashg-redhat/rhdh-on-eks\">repo</a>.</p>\n<p>Lets get right into it.</p>\n<p>As in many previous posts where I used EKS I'm going to use terraform to provision EKS cluster. You can find the terraform scripts <a href=\"https://github.com/rprakashg-redhat/rhdh-on-eks/tree/main/deploy/infra\">here</a>.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">terraform init\nterraform plan\nterraform apply --var \"name=toolscluster\"</code></pre></div>\n<p>Once the cluster is provisioned we need to install an Ingress controller. There are variety of options out there but for the purposes of this post I decided to use HAProxy. HAProxy also provides an EKS addon that makes it super easy to install on to cluster but I'm going to use helm to install HAProxy controller on the cluster.</p>\n<p>You can download the kubeconfig file to authenticate with the cluster by running command below.</p>\n<p>note: <em>Be sure to make sure region value matches to one you specified when cluster was created.</em></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">aws eks update-kubeconfig --region us-west-2 --name \"toolscluster\"</code></pre></div>\n<p>First we are going to add the HAProxy helm chart repo by running command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">helm repo add haproxytech https://haproxytech.github.io/helm-charts</code></pre></div>\n<p>Update helm charts by running <code class=\"language-text\">helm repo update</code> and install the HAProxy ingress controller by running command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">helm install haproxy-kubernetes-ingress haproxytech/kubernetes-ingress \\\n  --create-namespace \\\n  --namespace haproxy-controller \\\n  --set controller.service.type=LoadBalancer</code></pre></div>\n<p>Check if the ALB is provisioned successfully on AWS by running <em><code class=\"language-text\">kubectl describe service haproxy-ingress -n haproxy-controller</code></em>. You should see full DNS name of your ALB provisioned in Amazon for public IP. If it shows pending that means some issue which you should see in the status section.</p>\n<p>Next thing we are going to do is to update the route 53 hosted zone to create an A record so we can have a friendly URL like so <em><code class=\"language-text\">devhub.sandbox2841.opentlc.com</code></em>. Basically this is going to act as an alias for routing traffic to the ALB provisioned by HAProxy Ingress controller. See screenshopt below</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/006e0d3131e87b186c82c19ea6463e89/18c33/r53updates.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABGklEQVR42pWS+06DMBjFef/H8gl08ZKYsDllUjagtPTG9XjaTeMfLmrJL18bei4JZDebHHePz9g8POF2c498t4c4HlFWglR/5r0UeCsOyE6NRFWVUErBBw8XAsIw/BtPnXMO2WtZ4SAqnJoWTdui5tR9DxXR+sznmXQ8/wgLxZmVNKtbeRH3NDPQ1id8uCR7n4j7gW0S45jmyDlOEyzbpYbGGEjZwViLQIPvDMMITyPLd1FgjIWzZ2EiBnEqpXk/JLJpmilwKem3ta7rV6OVz7IsyXC7e0mFolcWmHLkV22bBss8R9VVVhrE+54mcc00j/uiKJJ+ZlAmhUCx2+JAOhq7roOV8irqxHBq4j1d15D8ZYo8h9jv4bXCB3NJZmPOVaChAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"r53update\"\n        title=\"\"\n        src=\"/static/006e0d3131e87b186c82c19ea6463e89/5a190/r53updates.png\"\n        srcset=\"/static/006e0d3131e87b186c82c19ea6463e89/772e8/r53updates.png 200w,\n/static/006e0d3131e87b186c82c19ea6463e89/e17e5/r53updates.png 400w,\n/static/006e0d3131e87b186c82c19ea6463e89/5a190/r53updates.png 800w,\n/static/006e0d3131e87b186c82c19ea6463e89/c1b63/r53updates.png 1200w,\n/static/006e0d3131e87b186c82c19ea6463e89/29007/r53updates.png 1600w,\n/static/006e0d3131e87b186c82c19ea6463e89/18c33/r53updates.png 1775w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Create a namespace <code class=\"language-text\">tools</code> on the cluster where we will install developer hub components by running <em><code class=\"language-text\">kubectl create namespace tools</code></em></p>\n<p>Next thing we are going to do is download redhat pull secret file from <em>cloud.redhat.com</em> and save it locally as pull-secret.txt file then create a kubernetes secret as shown below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl create secret generic rhdh-pull-secret \\\n  -n tools \\\n  --from-file=.dockerconfigjson=pull-secret.txt \\\n  --type=kubernetes.io/dockerconfigjson</code></pre></div>\n<p>Path the default service account to be able to pull images from redhat registries using the kubernetes secret we created earlier by running command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl patch sa default -n tools -p '{\"imagePullSecrets\": [{\"name\": \"rhdh-pull-secret\"}]}'</code></pre></div>\n<p>Next thing we are going to do is create a kubernetes secret to store all sensitive configurations we want to use in the app config file for backstage. See below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl create secret generic rhdh-secrets \\\n  -n tools \\\n  --from-literal=AUTH_OKTA_CLIENT_ID=${AUTH_OKTA_CLIENT_ID} \\\n--from-literal=AUTH_OKTA_CLIENT_SECRET=${AUTH_OKTA_CLIENT_SECRET} \\\n--from-literal=AUTH_OKTA_DOMAIN=${AUTH_OKTA_DOMAIN} \\\n--from-literal=AUTH_OKTA_ADDITIONAL_SCOPES=${AUTH_OKTA_ADDITIONAL_SCOPES} \\\n--from-literal=GITLAB_TOKEN=${GITLAB_TOKEN} \\\n--from-literal=GITLAB_APP_APP_ID=${GITLAB_APP_APP_ID} \\\n--from-literal=GITLAB_APP_CLIENT_ID=${GITLAB_APP_CLIENT_ID} \\\n--from-literal=GITLAB_APP_CLIENT_SECRET=${GITLAB_APP_CLIENT_SECRET} \\\n--from-literal=GITHUB_APP_APP_ID=${GITHUB_APP_APP_ID} \\\n--from-literal=GITHUB_APP_CLIENT_ID=${GITHUB_APP_CLIENT_ID} \\\n--from-literal=GITHUB_APP_CLIENT_SECRET=${GITHUB_APP_CLIENT_SECRET} \\\n--from-literal=GITHUB_ORG=${GITHUB_ORG} \\\n--from-literal=GITHUB_APP_WEBHOOK_URL=${GITHUB_APP_WEBHOOK_URL} \\\n--from-literal=GITHUB_APP_WEBHOOK_SECRET=${GITHUB_APP_WEBHOOK_SECRET} \\\n--from-literal=GITHUB_TOKEN=${GITHUB_TOKEN} \\\n--from-literal=BACKSTAGE_AWS_ACCOUNT_ID=${BACKSTAGE_AWS_ACCOUNT_ID} \\\n--from-literal=AWS_ACCESS_KEY_ID=${BACKSTAGE_AWS_ACCESS_KEY_ID} \\\n--from-literal=AWS_SECRET_ACCESS_KEY=${BACKSTAGE_AWS_SECRET_ACCESS_KEY} \\\n--from-literal=TECHDOCS_AWSS3_BUCKET_NAME=${TECHDOCS_AWSS3_BUCKET_NAME} \\\n--from-literal=TECHDOCS_AWSS3_BUCKET_URL=${TECHDOCS_AWSS3_BUCKET_URL} \\\n--from-literal=AWS_REGION=${AWS_REGION} \\\n--from-literal=EKS_CLUSTER_URL=${EKS_CLUSTER_URL} \\\n--from-literal=EKS_CLUSTER_NAME=${EKS_CLUSTER_NAME} \\\n--from-literal=BACKSTAGE_ROLE_ARN_TO_ASSUME=${BACKSTAGE_ROLE_ARN_TO_ASSUME} \\\n--from-literal=AWS_EXTERNAL_ID=${AWS_EXTERNAL_ID} \\\n--from-literal=EKS_SA_TOKEN=${EKS_SA_TOKEN} \\\n--from-literal=ARGOCD_USER_ID=${ARGOCD_USER_ID} \\\n--from-literal=ARGOCD_USER_PWD=${ARGOCD_USER_PWD}</code></pre></div>\n<p>I was testing OKTA, GITHUB, GITLAB mine looks pretty large. Approach I took was to keep a local env file that contains all the values specific to my environment\nand I would run command <em><code class=\"language-text\">export $(cat local.env | xargs)</code></em>  before creating the secret.</p>\n<p>Originally for Auth I used OKTA and eventually landed on using GitHub. Steps for creating app in github is pretty well documented so I'm not going to cover that. Once you create the app in github you will endup having to create a private key as well. Download the private key in to your local machine and create another kubernetes secret to store the private key for your app as you will need to specify that in the app config for backstage</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl create secret generic -n tools gh-app-key \\\n--from-file=GITHUB_APP_PRIVATE_KEY=\"/Users/rgopinat/keys/demo-rhdh.2024-02-26.private-key.pem\"</code></pre></div>\n<p>For Kubernetes plugin I downloaded the EKS CA Certificate from aws console and saved locally to create another secret to store that EKC CA data as shown below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl create secret generic -n tools eks-ca-data \\\n--from-file=EKS_CA_DATA=eks-ca.txt</code></pre></div>\n<p>Next I do have to tell backstage about all these secrets I've created so backstage can load them as environment variables and we can do that by updating a section in values file as shown below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> extraEnvVarsSecrets:\n    - rhdh-secrets\n    - gh-app-key\n    - eks-ca-data</code></pre></div>\n<p>Next thing we are going to do is to go ahead and create the app config configmap. You can see a copy of mine <a href=\"https://github.com/rprakashg-redhat/rhdh-on-eks/blob/main/deploy/rhdh/developer-hub-appconfig.yaml\">here</a></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl apply -f deploy/rhdh/developer-hub-appconfig.yaml</code></pre></div>\n<p>It is worth talking a little bit about this snippet of yaml below in the app config. Basically this is going to tell backstage to automatically ingest entities from github and the two static locations you see are one for golden path templates and the other one is for ingesting all APIs from repo where I'm storing all API specifications</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">catalog:\n    import:\n    entityFileName: catalog-info.yaml\n    rules:\n    - allow: [Component, System, API, Resource, Location, Domain, Template]\n    locations:\n    - type: url\n        target: https://github.com/rprakashg-redhat/rhdh-templates/blob/main/all-templates.yaml\n    - type: url\n        target: https://github.com/rprakashg-redhat/apis/blob/main/all-apis.yaml</code></pre></div>\n<p>At this point we are ready to install redhat developer hub. We provide a Helm chart to do just that. We will add the helm chart repo with this command <em><code class=\"language-text\">helm repo add openshift-helm-charts https://charts.openshift.io/</code></em>. Next we need to download the values yaml so we can review and customize them to fit to this EKS installation.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">helm show values openshift-helm-charts/redhat-developer-hub > values.yaml</code></pre></div>\n<p>Customize the values yaml. You can see the version I used for my installation <a href=\"https://github.com/rprakashg-redhat/rhdh-on-eks/blob/main/deploy/rhdh/values.yaml\">here</a>.\nFew other customizations to values file that are worth calling out are below</p>\n<ol>\n<li>Specified the custom hostname <code class=\"language-text\">devhub.sandbox2841.opentlc.com</code> under section global of values yaml</li>\n<li>Since I was using HAProxy ingress under upstream section I had to enable ingress and specify className as shown in the snippet below</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">ingress:\n    enabled: true\n    host: \"{{ .Values.global.host }}\"\n    className: haproxy</code></pre></div>\n<ol start=\"3\">\n<li>Tell developer hub about the app config by updating the extraAppConfig section under upstream->backstage section in values file as shown in snippet below</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">extraAppConfig:\n- configMapRef: \"developer-hub-appconfig\"\n  filename: \"developer-hub-appconfig.yaml\"</code></pre></div>\n<ol start=\"4\">\n<li>Update podSecurityContext section under upstream->backstage and include below config</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">podSecurityContext:\n    runAsUser: 1001\n    runAsGroup: 1001\n    fsGroup: 1001</code></pre></div>\n<ol start=\"5\">\n<li>Update podSecurityContext section under upstream->postgresql->primary section in values file as shown in snippet below</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">podSecurityContext:\n    enabled: true\n    fsGroup: 26\n    runAsUser: 26</code></pre></div>\n<p>and set volume permissions to enabled as shown in snippet below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">volumePermissions:\n    enabled: true</code></pre></div>\n<ol start=\"6\">\n<li>Lastly I had to set route enabled to false since I'm not running developer hub on Openshift.</li>\n</ol>\n<p>At this point we can go ahead and install redhat developer hub by running command <em><code class=\"language-text\">helm upgrade --namespace tools -i developer-hub -f values.yaml openshift-helm-charts/redhat-developer-hub</code></em></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">helm upgrade --namespace tools -i developer-hub -f values.yaml openshift-helm-charts/redhat-developer-hub</code></pre></div>\n<p>Browse to <em><code class=\"language-text\">https://devhub.sandbox2841.opentlc.com</code></em> from a browser and login and you will be in home page of developer hub as shown in screen capture below</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/28b17bc865fbf5cf1eddb74c31dd8d3f/c2d13/devhubhome.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 34%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABFUlEQVR42oWQyU7DMBCG/UpIXErpQuGZWKQ0cKRcCqqKql7CI3FFLEpFgKaBRNmc2Fl+xgYkhNJi6dN4xv5/z5i1trZxa57BPr/AnTHEvXmKx6GJh8MjPB2fwDYMLCh/GY3wNh7DvbyCN5ngfT6Hb1kIrBuEiusp4tkMrNMf4HXpQpYlZFVBUBQUZV03kgqBKMv0XUVWSEhAo3SsPziA4ziQUqCmQk0iHRuo6LEoDOEHAXiaIuMcJdWq77OiKMDanR4W9rNOyAqblhLmea7hZKbIqNufmoLt7HaxWnlaoLvbwG/DdbBWuwvPXSJOOdJc/tuhoD9cZ6bOWG9vH77/oUcW8stQ/Yuk/V+UIEkSPWqTWRTH+ATO9QPErE/ovAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"devhubhome\"\n        title=\"\"\n        src=\"/static/28b17bc865fbf5cf1eddb74c31dd8d3f/5a190/devhubhome.png\"\n        srcset=\"/static/28b17bc865fbf5cf1eddb74c31dd8d3f/772e8/devhubhome.png 200w,\n/static/28b17bc865fbf5cf1eddb74c31dd8d3f/e17e5/devhubhome.png 400w,\n/static/28b17bc865fbf5cf1eddb74c31dd8d3f/5a190/devhubhome.png 800w,\n/static/28b17bc865fbf5cf1eddb74c31dd8d3f/c1b63/devhubhome.png 1200w,\n/static/28b17bc865fbf5cf1eddb74c31dd8d3f/29007/devhubhome.png 1600w,\n/static/28b17bc865fbf5cf1eddb74c31dd8d3f/c2d13/devhubhome.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Navigating to APIs you can see that petstore api was ingested into catalog automatically from the repo. Currently repo has only one API spec but as we add more API specs they would be automatically ingested into the catalog which is super cool.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b38495c951fb65d0bdb74d3f88c140f0/c2d13/apis.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABdklEQVR42p2Qy07bUBRF/VNIlQptESp/AA0tj34AUyJAoAqplfgixp12yis8bNlJnIcdnKTx9bUdJ1mca2AAUiYMts5jn7vvPsfy601adoP7Kw+71sC5kfzCwZXYtNslGvdtfLtDyw3w3ZCWF+LXHwQR3WZEz39Cx+tirSx84F/lO5ffKpxXNqjt/OT6xya1tXXupLY3t3C2d6jv7uLv7dGuVukeHBCenNA/PSX4/Qf38BdBdR91fIS1uPSZjuuSDf+jej1G3QAd9ckGQ9K+iQPhhuRKkcfxE0Yx2WjETCfUxdXZ3wtubpulhrW8skonCCimU3SWEUkzLwqmQD6ZlLGYzlBpxmQ2o5BeonU5W86Mc7JUk0odJ4k4/LSM53kocTAUJ0qamZBpmr6CEXnJtRF8rk1UiSaRdyYXwS84jsNA1uvJyvpZ8L2wPsoNwzBkPB6Xv5hfDZHn+VzMEzOc3PArURRRyN2MZUOUa8gJ5uFl7q1YrBSPxtpBSg4C1TgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"apis\"\n        title=\"\"\n        src=\"/static/b38495c951fb65d0bdb74d3f88c140f0/5a190/apis.png\"\n        srcset=\"/static/b38495c951fb65d0bdb74d3f88c140f0/772e8/apis.png 200w,\n/static/b38495c951fb65d0bdb74d3f88c140f0/e17e5/apis.png 400w,\n/static/b38495c951fb65d0bdb74d3f88c140f0/5a190/apis.png 800w,\n/static/b38495c951fb65d0bdb74d3f88c140f0/c1b63/apis.png 1200w,\n/static/b38495c951fb65d0bdb74d3f88c140f0/29007/apis.png 1600w,\n/static/b38495c951fb65d0bdb74d3f88c140f0/c2d13/apis.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>To create new components we can click on create and you'll see all golden path templates ingested into the catalog as shown in screen capture below. I currently have just two.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6ae8ea059068618f507cd176d129edee/c2d13/templates.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABs0lEQVR42m2Qy27TQBhG/VBIbBqgquANQAWVbrhIoG4bUbW0UpFA6hq2fYAuoEtWSGxgC7Q4ECdxLnYutusk+Jo49uEfR0ICOtLRN/+MdOazNavTo2/aNOuCIfzoYnxr0NI79GTuNvp0jD5WY4BtjrAE23Sw2h52x2Nk+TjWBa4wbA/R1q5c5fP6Pc4fPER/8pSfjx5T27hP7fYd6ut3aci+ublJd2sLe3ubQbXKcGcH9/AF7stXtPcPae8eMKo+I9p/jlapXKf+5SvDVoux3WcyGgkOkecROpIXPtF4TDydEqscT8o5nU7whi4fPum8/3jGd71N7I/RVtdu0bEswijiV7ag+/Ydxt4e3vExwckJ1us39I6O8CSD01PSJCHNMtL5nJlkUeQU5GT5ojzTVq6tUjcMXGnjSCun0cQ9O2dimiSDAa6uCzXiXo+565Km6V8k8kAsqFRzKbSkoRoiaRkmKYuCcuVCPJuRSJMsz8mL4j/hv2grlRv4vs9c6sZxTCKofSaSROTqExdKOM+kScpMHlBcJlPn8g9vlsKlYFlbicMw/EMQBGVG0XJW95fJgjDkN0lvONQfEOenAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"templates\"\n        title=\"\"\n        src=\"/static/6ae8ea059068618f507cd176d129edee/5a190/templates.png\"\n        srcset=\"/static/6ae8ea059068618f507cd176d129edee/772e8/templates.png 200w,\n/static/6ae8ea059068618f507cd176d129edee/e17e5/templates.png 400w,\n/static/6ae8ea059068618f507cd176d129edee/5a190/templates.png 800w,\n/static/6ae8ea059068618f507cd176d129edee/c1b63/templates.png 1200w,\n/static/6ae8ea059068618f507cd176d129edee/29007/templates.png 1600w,\n/static/6ae8ea059068618f507cd176d129edee/c2d13/templates.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Hope this was helpful. As always feel free to reach out to me if you have questions about this post or want to learn more about RedHat developer hub. I'm so excited about RedHat developer hub and backstage and the value it can add to development teams also what RedHat is doing in this space.</p>\n<p>Thanks,\nRam</p>","id":"cb7c86c4-bde9-59d7-b4ff-acc106a6001c","frontmatter":{"title":"Installing RedHat Developer Hub on Amazon Elastic Kubernetes Service (EKS)","date":"March 16, 2024","tags":["backstage","idp","kubernetes","eks","amazon","redhat"],"author":"Ram Gopinathan"},"fields":{"slug":"/installing-rhdh-on-eks/"}}}]}},"pageContext":{"limit":3,"skip":9,"numPages":16,"currentPage":4}},"staticQueryHashes":["1611934721","2366241629"],"slicesMap":{}}