{"componentChunkName":"component---src-templates-post-list-js","path":"/4/","result":{"data":{"site":{"siteMetadata":{"title":"rprakashg.github.io","author":"RAM GOPINATHAN"}},"allMarkdownRemark":{"totalCount":37,"edges":[{"node":{"excerpt":"I thought I'll do this post to document some of the things I ran into when working with RedHat Developer Hub. I'm planning to keep this post updated as I learn more from my own research as well as…","html":"<p>I thought I'll do this post to document some of the things I ran into when working with RedHat Developer Hub. I'm planning to keep this post updated as I learn more from my own research as well as from customer implementations. My hope is that others will find this useful.</p>\n<p><strong>First</strong> thing I want to cover here is when you uninstall RedHat Developer Hub using helm uninstall both the PV and PVC used by Postgresql chart will still be on the cluster, because of this when you attempt to re-install the developer hub using Helm chart you will find that the Postgresql pod will not come up. Init container that configures volume permissions will fail. You are going to see this error <strong><code class=\"language-text\">chmod: changing permissions of '/var/lib/pgsql/data/userdata': Operation not permitted</code></strong> when you look at the logs for Postgresql pod. If you ran into this issue you can just simply delete the pvc first by running this command <em><code class=\"language-text\">kubectl delete pvc data-developer-hub-postgresql-0 -n tools</code></em> also make sure the associated pv is also deleted by running <em><code class=\"language-text\">kubectl get pv</code></em> and then re-install developer hub. Unfortunately we do not have much control over this as we embed the bitnami postgresql chart into the helm chart for installing developer hub</p>\n<p><strong>Second</strong> When you configure Redhat Developer Hub instance to use OKTA auth after successfully logging in you will see an error <strong><code class=\"language-text\">User not found</code></strong> this is mainly because the backstage will check to ensure a matching user entity exists in the catalog using signin resolvers. Many auth provider plugins automatically will create user entities in the catalog unfortunately Okta and Bitbucket plugins do not do this yet. One of my coworkers wrote this <a href=\"https://github.com/redhat-na-ssa/rhdh-bitbucket-auth/blob/main/Readme.Md\">post</a> that might be helpful if you are working with bitbucket or okta plugins for auth, although the article is for bitbucket, it should also work for okta.</p>\n<p><strong>Third</strong> Backstage can automatically build and publish your technical documentation and best practice is to build publish externally in a CICD pipeline. See an example of this <a href=\"https://github.com/rprakashg-redhat/petstore-go/blob/main/.github/workflows/techdocs.yaml\">here</a> backstage can serve static HTML files directly as you can see from the screen capture below</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3ccda603cfa22e0df4d92aa81571f9c7/c2d13/techdocs.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNElEQVR42p2Py07CQBSG+1a68EYI8U1AAXHPjpCw0bABfC0TLyTS0oKUMuVSoZ3OpSx+TxtDNCFBXXw5Z85MvvmPYVpT2C8OBs9DvBHWK/VPFuz+CJ45QTB24Q7e4fTHcC0P7nAG12aYOPOM6Yig6lOdOR6MwtExHssVDGq36JeuYFI1K1VYxSKs0jXschWjmxqm9Tq8RgN+s4lFq4Wg3ca600HY7SJ66CGms7i/g3F6nsOCMSRxDLHeYLNcZqgogtyEUGEIHXEIgocRtBBIdkgkUkLHYjc38oVLuDMGoTSk1mC+DzafQyoFlSRQNDvEcrVCSAE0vc8SBsEHttstBP3AKG1K2qtU+ke+hEEmlBQ/Tlf/pywTnpxd/BBqWuE37JOlcyOXL2D1TZhecM4Psk8WcY5PgqhFMBLc6UcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"techdocs\"\n        title=\"\"\n        src=\"/static/3ccda603cfa22e0df4d92aa81571f9c7/5a190/techdocs.png\"\n        srcset=\"/static/3ccda603cfa22e0df4d92aa81571f9c7/772e8/techdocs.png 200w,\n/static/3ccda603cfa22e0df4d92aa81571f9c7/e17e5/techdocs.png 400w,\n/static/3ccda603cfa22e0df4d92aa81571f9c7/5a190/techdocs.png 800w,\n/static/3ccda603cfa22e0df4d92aa81571f9c7/c1b63/techdocs.png 1200w,\n/static/3ccda603cfa22e0df4d92aa81571f9c7/29007/techdocs.png 1600w,\n/static/3ccda603cfa22e0df4d92aa81571f9c7/c2d13/techdocs.png 2560w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>When I was first trying out the techdocs capability configuring backstage to do a build and publish locally worked fine and could see the static HTML files directly from backstage but when I switch the configuration to external docs did not render in backstage. I was getting HTTP 404 errors and the reason for this was when you publish using techdocs CLI as shown in the snippet below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">- name: Publish docs site\n  run: | \n  techdocs-cli publish --publisher-type awsS3 \\\n      --storage-name $TECHDOCS_S3_BUCKET_NAME \\\n      --entity $NAMESPACE/$KIND/$REPO_NAME</code></pre></div>\n<p>I was using k8s namespace as $NAMESPACE value. S3 router expects this to be <strong><code class=\"language-text\">default</code></strong> and also Kind should be Component matching the entity in catalog. After I fixed the pipeline I'm now able to view the static HTML files built in a CICD pipeline from backstage as you can see from the screen capture above.</p>\n<p>Hope this helps,\nRam</p>","id":"3f820f87-5a6f-5410-b55e-38c4dd0018bf","frontmatter":{"title":"Things you should know about RedHat Developer Hub","date":"March 16, 2024","tags":["backstage","idp","kubernetes","redhat"],"author":"Ram Gopinathan"},"fields":{"slug":"/rhdh-things-to-know/"}}},{"node":{"excerpt":"When you use HAProxy Ingress with ArgoCD on Kubernetes you are going to run into this error 'ERR_TOO_MANY_REDIRECTS' when you browse to the ArgoCD server URL. I lost of few hours trying to figure out…","html":"<p>When you use HAProxy Ingress with ArgoCD on Kubernetes you are going to run into this error 'ERR_TOO_MANY_REDIRECTS' when you browse to the ArgoCD server URL. I lost of few hours trying to figure out what the cause is and wanted to write this up with the hope that it saves some one else time later and also a note for myself so I don't forget exactly what I did to solve the issue.</p>\n<p>I installed ArgoCD server on my EKS cluster following the instructions in the ArgoCD docs using the command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectcl create namespace gitops\nkubectl apply -n gitops -f https://raw.githubusercontent.com/argoproj/argo-cd/master/manifests/install.yaml</code></pre></div>\n<p>After that I created an Ingress Resource by running the command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd\n  namespace: gitops\nspec:\n  ingressClassName: haproxy\n  rules:\n  - host: gitops.sandbox2841.opentlc.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: argocd-server\n            port:\n              name: https\n        path: /\n        pathType: Prefix\n  tls:\n  - hosts:\n    - gitops.sandbox2841.opentlc.com\nEOF</code></pre></div>\n<p>Browse to <code class=\"language-text\">https://gitops.sandbox2841.opentlc.com</code> and started noticing 'ERR_TOO_MANY_REDIRECTS'. reason why this was happening is because HAProxy by default terminates SSL and forwards HTTP to backend service and ArgoCD server when it sees an HTTP traffic it automatically redirects to HTTPS. This results in an infinite loop and eventually HAProxy gives up and you see that error in browser.\nSo to fix this problem I simply had to add an annotation <code class=\"language-text\">ingress.kubernetes.io/ssl-passthrough: \"true\"</code> to the ingress resource so HAProxy instead of terminating HTTPS traffic at the proxy will do an SNI passthrough.</p>\n<p>Updated Ingress resource as shown below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd\n  namespace: gitops\n  annotations:\n    ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  ingressClassName: haproxy\n  rules:\n  - host: gitops.sandbox2841.opentlc.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: argocd-server\n            port:\n              name: https\n        path: /\n        pathType: Prefix\n  tls:\n  - hosts:\n    - gitops.sandbox2841.opentlc.com\nEOF</code></pre></div>\n<p>Grab the admin password from kubernetes secret by running command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl get secrets argocd-initial-admin-secret -n gitops -o jsonpath=\"{.data.password}\" | base64 -d | pbcopy</code></pre></div>\n<p>Browse to <code class=\"language-text\">https://gitops.sandbox2841.opentlc.com</code> enter admin for username and paste the password obtained from previous step and you will be logged into ArgoCD server successfully as shown in screen capture below</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9ea613f81a97d2d066809a1501b42380/12e1b/argocd.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABhElEQVR42pWQu0oDURCGt40gWlmKCqKNgkSENIIvIHa2lj6ZLyAoKSw06qrEJOayJCa7m83er7nauMn+zjkJooVCio85Z+acj5kRXD+Cb/uwTR+GHcJ0Quh2BMvrw49GHMMK4dDdDUdw/D48FoMBTJdyFG1WC4b0pgvh5OwcmeNTiPkioh4V/ICKERUDxHGMeDym+IkxxfFkgiRJgBkJ3enwnUuSCYTV9CFS6zu4uMwiX2si91rBQ6HK6bgB4cMMurDDHsyIuu6PEPSHxAjhgM69ISec5YW1gyMsbu4he/8Mb/CBtkMSL+TIlsdpme4U4zeNjo1iQ0WxrqKuWTwnrOxkuPDqVoRJXTR1GzJ9Zigk+wuV9i6pBipNDdVWBzXF4DlhaSuN1MYuru9EWDRSy3D+FXEZwSZhXVdIxmjqDjRakbC8vY+FOYSsc0nVaVQZ5XeVutOmUdZQaijzCdlIdc0kSZskCt5IcCMWkM290B5lSIo+f4c/5Wzkp3IdjyWJ/2Nr+AJ7vXGtXdpr+wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"argocd\"\n        title=\"\"\n        src=\"/static/9ea613f81a97d2d066809a1501b42380/5a190/argocd.png\"\n        srcset=\"/static/9ea613f81a97d2d066809a1501b42380/772e8/argocd.png 200w,\n/static/9ea613f81a97d2d066809a1501b42380/e17e5/argocd.png 400w,\n/static/9ea613f81a97d2d066809a1501b42380/5a190/argocd.png 800w,\n/static/9ea613f81a97d2d066809a1501b42380/c1b63/argocd.png 1200w,\n/static/9ea613f81a97d2d066809a1501b42380/29007/argocd.png 1600w,\n/static/9ea613f81a97d2d066809a1501b42380/12e1b/argocd.png 1607w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Hope this helps,</p>\n<p>Thanks,\nRam</p>","id":"2b8c0cb4-6808-5d23-806d-7f23131e082c","frontmatter":{"title":"Fixing error 'ERR_TOO_MANY_REDIRECTS' when using HAProxy with ArgoCD","date":"March 14, 2024","tags":["gitops","argocd","Kubernetes"],"author":"Ram Gopinathan"},"fields":{"slug":"/haproxy-ingress-with-argocd/"}}},{"node":{"excerpt":"I've been hearing a lot about Crossplane for over a year now and recently I got a chance to try it out, specifically resource composition capabilities. In this post I'll go over how I used these…","html":"<p>I've been hearing a lot about Crossplane for over a year now and recently I got a chance to try it out, specifically resource composition capabilities. In this post I'll go over how I used these capabilities to build a system that automates provisioning of EKS clusters in AWS. I hope you are just as excited as I am. What I liked the most is that crossplane allows us to extend kubernetes without really writing single line of go code or having to be expert in kubernetes operators. Everything I'm going to cover here is in <a href=\"https://github.com/rprakashg-redhat/crossplane-eks\">this</a> github repo. Lets jump right into it.</p>\n<p>Provision an EKS cluster by running terraform scripts included in the infra directory in the github repo, this cluster is going to act as a tools cluster where I will be installing crossplane.</p>\n<p>Create the tools cluster by running terraform commands below</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">cd infra\n\nterraform init\nterraform plan\nterraform apply</code></pre></div>\n<p>After terraform commands are completed successfully we can see that the cluster was provisioned and is active from the screen capture below.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/63b67/ekscrossplane.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 14.500000000000002%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAjUlEQVR42k3MSw7CMAwE0N7/UhyBLUuEEKI0bYiTOr8mQ2JoxeLJ9tjyoOYFz9eE2/2B3k9q/qs/6mtued/ts5gURqUxXs8YLycMMSWEEKANIeUsOHjkbZM8xohSCmqpWD03rt0kyXYhJmRPyLxgqLWCmeUhOQfbGKKjkrXSd5reWIyGsXRkndysDMseH5B35hYtJ1h7AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"crossplaneeks\"\n        title=\"\"\n        src=\"/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/5a190/ekscrossplane.png\"\n        srcset=\"/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/772e8/ekscrossplane.png 200w,\n/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/e17e5/ekscrossplane.png 400w,\n/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/5a190/ekscrossplane.png 800w,\n/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/c1b63/ekscrossplane.png 1200w,\n/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/29007/ekscrossplane.png 1600w,\n/static/a0a0d5ea445cb5a8406ce46d2b5ba3e1/63b67/ekscrossplane.png 2447w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Install crossplane using helm chart. This is pretty well documented <a href=\"https://docs.crossplane.io/v1.14/software/install/\">here</a></p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">helm repo update\n\nhelm install crossplane \\\n    crossplane<span class=\"token punctuation\">-</span>stable/crossplane \\\n    <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>namespace crossplane<span class=\"token punctuation\">-</span>system \\\n    <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>create<span class=\"token punctuation\">-</span>namespace</code></pre></div>\n<p>Lets verify that the crossplane pods are running and healthy</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">kubectl get pods <span class=\"token punctuation\">-</span>n crossplane<span class=\"token punctuation\">-</span>system</code></pre></div>\n<p>and we can see that all crossplane system pods are up and running</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5d70a69e0bc5d09f64c1d32e106a8699/2c118/crossplanesystem.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAfUlEQVR42kXKuw6CMBiA0b+WqfciVYjBiPImJmIY3dT3f47PhsXhbEeGYaA/HjC2ReyjWlBhrZ4ov6DjSj5/sP2LePpi7jPt22OuFlMMKSWcc3jv6boOmaaJcRwppbBvQxXZ7RQistFa12zIOTHfLgQXaKQh+kiOeTtK/f8P87kw0ePCGIcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"crossplanesystem\"\n        title=\"\"\n        src=\"/static/5d70a69e0bc5d09f64c1d32e106a8699/5a190/crossplanesystem.png\"\n        srcset=\"/static/5d70a69e0bc5d09f64c1d32e106a8699/772e8/crossplanesystem.png 200w,\n/static/5d70a69e0bc5d09f64c1d32e106a8699/e17e5/crossplanesystem.png 400w,\n/static/5d70a69e0bc5d09f64c1d32e106a8699/5a190/crossplanesystem.png 800w,\n/static/5d70a69e0bc5d09f64c1d32e106a8699/c1b63/crossplanesystem.png 1200w,\n/static/5d70a69e0bc5d09f64c1d32e106a8699/2c118/crossplanesystem.png 1505w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Install following crossplane AWS providers shown below. Providers enable crossplane to provision infrastructure on an external service. For more info on other providers including the AWS ones listed below checkout <a href=\"https://marketplace.upbound.io/providers\">https://marketplace.upbound.io/providers</a></p>\n<ul>\n<li><a href=\"https://marketplace.upbound.io/providers/upbound/provider-aws-ec2/v0.47.1\">ec2</a></li>\n<li><a href=\"https://marketplace.upbound.io/providers/upbound/provider-aws-kms/v0.47.1\">kms</a></li>\n<li><a href=\"https://marketplace.upbound.io/providers/upbound/provider-aws-iam/v0.47.1\">iam</a></li>\n<li><a href=\"https://marketplace.upbound.io/providers/upbound/provider-aws-eks/v0.47.1\">eks</a></li>\n</ul>\n<p>Install a go templating composition function which I'll use when defining resource composition for the composite API we will define later in this post. For more information about this function check out the upbound marketplace <a href=\"https://marketplace.upbound.io/functions/crossplane-contrib/function-go-templating/v0.4.1\">https://marketplace.upbound.io/functions/crossplane-contrib/function-go-templating/v0.4.1</a></p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">cat &lt;&lt;EOF <span class=\"token punctuation\">|</span> kubectl apply <span class=\"token punctuation\">-</span>f <span class=\"token punctuation\">-</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> pkg.crossplane.io/v1beta1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Function\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> function<span class=\"token punctuation\">-</span>go<span class=\"token punctuation\">-</span>templating\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">package</span><span class=\"token punctuation\">:</span> xpkg.upbound.io/upbound/function<span class=\"token punctuation\">-</span>go<span class=\"token punctuation\">-</span>templating<span class=\"token punctuation\">:</span>v0.4.1\nEOF</code></pre></div>\n<p>verify that the function was installed successfully and running healthy.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">kubectl get function</code></pre></div>\n<p>Setup cloud provider credentials. Run command below to create a credentials file. Be sure to create an IAM account with required privileges and set the environment variables referenced in the below command with respective values that match your environment.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">echo \"[default] \\naws_access_key_id = ${AWS_ACCESS_KEY_ID}\\naws_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\" > aws-credentials.txt</code></pre></div>\n<p><strong><em>Be sure to not checkin the <code class=\"language-text\">aws-credentials.txt</code> file to github by including an entry in your <code class=\"language-text\">.gitignore</code> file to ignore it</em></strong></p>\n<p>Create a kubernetes secret from <code class=\"language-text\">aws-credentials.txt</code> file by running command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl create secret \\\n    generic aws-secret \\\n    -n crossplane-system \\\n    --from-file=creds=./aws-credentials.txt</code></pre></div>\n<p>Create a provider config resource to customize the settings of the AWS provider. Basically this is how we tell the crossplane AWS provider how to authenticate with AWS.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">cat &lt;&lt;EOF <span class=\"token punctuation\">|</span> kubectl apply <span class=\"token punctuation\">-</span>f <span class=\"token punctuation\">-</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> aws.upbound.io/v1beta1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ProviderConfig\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>config\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">credentials</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">source</span><span class=\"token punctuation\">:</span> Secret\n    <span class=\"token key atrule\">secretRef</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> crossplane<span class=\"token punctuation\">-</span>system\n      <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>secret\n      <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> creds\nEOF</code></pre></div>\n<h2>Create Composite Resource Definitions</h2>\n<p>Composite resource definitions (XRDs) allows us to define the schema for custom APIs. All schemas follow kubernetes custom resource definition <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#specifying-a-structural-schema\">OpenAPIv3 structural schema</a>. Since I want the cluster to be provisioned within its own VPC with public and private subnets and route tables I decided to create 2 custom APIs. 1) Virtualnetwork (Creates VPC with subnets and route tables, Internet Gateway, NAT Gateway, Routing rules). 2) EKSCluster (Creates EKS Clusters with managed nodegroups and virtual network). Both APIs can be found <a href=\"https://github.com/rprakashg-redhat/crossplane-eks/tree/main/apis\">here</a></p>\n<p>Create the XRDs by running the command below.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl apply -f apis/virtualnetwork.yaml\n\nkubectl apply -f apis/ekscluster.yaml</code></pre></div>\n<p>We can verify that the XRD's are created by running <code class=\"language-text\">kubectl get xrd</code></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/26d15dbdc340593d60e4ee19c52fd44b/54c3a/verifyxrd.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 9.499999999999998%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAhUlEQVR42j2MzQqCQBgAv9UFMXVX19T9gYTwED1DxxA61sEuvf9rTELQYRiYw0iMkR+JcejQ1RWT3vjlQxNf9POGO23U4YluH2TNSt6vZPeS7CaoQVHYAmssWmskpYT3nhDC7ony4DiOC9bNSN4yxQuVSbjhjNIdkrV7N4gRpN6HohCRP19FYjFx04gJTQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"verifyxrd\"\n        title=\"\"\n        src=\"/static/26d15dbdc340593d60e4ee19c52fd44b/5a190/verifyxrd.png\"\n        srcset=\"/static/26d15dbdc340593d60e4ee19c52fd44b/772e8/verifyxrd.png 200w,\n/static/26d15dbdc340593d60e4ee19c52fd44b/e17e5/verifyxrd.png 400w,\n/static/26d15dbdc340593d60e4ee19c52fd44b/5a190/verifyxrd.png 800w,\n/static/26d15dbdc340593d60e4ee19c52fd44b/c1b63/verifyxrd.png 1200w,\n/static/26d15dbdc340593d60e4ee19c52fd44b/54c3a/verifyxrd.png 1257w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>Create Compositions</h2>\n<p>Compositions are like a deployment template for provisioning a group of resources as single object. I've defined the composition logic for both virtualnetwork and ekscluster XRDs and will go ahead and create them. In summary all required steps for provisioning VPC with subnets and route tables are defined in the composition for virtualnetwork and all resources that need to be created to have a fully functional eks cluster are defined in the composition for ekscluster XRD. You can see in the composition file I'm basically defining all the managed resources defined in the providers and crossplane takes care of provisioning them in my AWS account. You'll also notice each managed resource has a <code class=\"language-text\">providerConfigRef</code> property that we are using to tell crossplane system how to authenticate with the AWS account which we configured earlier.</p>\n<p>You can also see in the composition for <code class=\"language-text\">ekscluster</code> XRD that I'm nesting the <code class=\"language-text\">VirtualNetwork</code> XRD instead of duplicating all the logic of provisioning VPC, Subnets, route tables etc. By moving that into a seperate XRD allows us to reuse it for other usecases as well</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">kubectl apply -f composition/virtualnetwork.yaml\nkubectl apply -f composition/ekscluster.yaml</code></pre></div>\n<p>We can see that composition resources are created successfully for our XRDs</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/54af83fc5bedb03e4ac1110a02240831/9ba38/composition.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 10.999999999999998%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAgUlEQVR42lXMywqCQBiA0R80yhoHZ8RM0HEEYVwG4i6Qrvj+L/Rl7lqc7ZEQAs65TVVdyLKcLiyUbuagrzT9m25YODczupgwxY1955BBkFyIjhE61RhjSJIE+UXe+zVssdagUkPpF7LqSawmCvchr18bWz9Q9s6ub5FxzU4xIvLnC5AoM3PDbbQMAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"compositions\"\n        title=\"\"\n        src=\"/static/54af83fc5bedb03e4ac1110a02240831/5a190/composition.png\"\n        srcset=\"/static/54af83fc5bedb03e4ac1110a02240831/772e8/composition.png 200w,\n/static/54af83fc5bedb03e4ac1110a02240831/e17e5/composition.png 400w,\n/static/54af83fc5bedb03e4ac1110a02240831/5a190/composition.png 800w,\n/static/54af83fc5bedb03e4ac1110a02240831/c1b63/composition.png 1200w,\n/static/54af83fc5bedb03e4ac1110a02240831/9ba38/composition.png 1329w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>Creating an EKS cluster</h2>\n<p>We can now create an EKS cluster by simply running command below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: stacks.aws.io/v1alpha1\nkind: EKSCluster\nmetadata:\n  name: crossplane-eks-cluster\nspec:\n  region: \"us-west-1\"\n  version: \"1.27\"\n  compute:\n    nodeGroups:\n    - name: default\n      instanceType: t3.medium\n      scaling:\n        minSize: 2\n        maxSize: 3\n        desiredSize: 2\n  availabilityZones:\n  - us-west-1a\n  - us-west-1c\n  networking:\n    name: crossplane-eks-cluster-vpc\n    cidr: 10.0.0.0/16\n    publicSubnet:\n      cidr:\n      - 10.0.3.0/24\n      - 10.0.4.0/24\n    privateSubnet:\n      cidr:\n      - 10.0.1.0/24\n      - 10.0.2.0/24\nEOF</code></pre></div>\n<p>To see a list of managed resources that are created and their status we can run <code class=\"language-text\">kubectl get managed</code></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b31cce763de8128348beb4a33a705cff/07d7d/managed.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.49999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABk0lEQVR42n2T2ZKCUBBD3ZUdFARcAMHSUuf/fy/D6ar7Mo4+pJqLTTrpXCd1XWu/32u3KxRm9xEP+cldwViT4ke7V636VqmqaiVJotlspslk8hlVVSmKIsVxpDTxDUnsKwzWisKNYi9SnuU6Ho/K89xA/3q9NrwRXq9XDcNgHwzjc9t16vtBTdupG9EPvcqq1Ha7NYX0Px4PdWMf5zfCsiwVhuGoMLYGKkAFZyounMLNZmO2V6vV//aLolCWZaYApGk67nMn3rNb3jHQ9315nmfgOQgCzefzd0JCcbuBBDI3gOpCa5rGfmfgYrH4HAqNp9PJwPPhcDB75/PZhmAb4rZtx932NmC5XH5PGflub4AzNlGJYiq7JlV29/Xa0IiNv3sELhiU4gByBn+9i+zHpexUcmYAZFhkBc4FKqfT6WdCppMaJM6qSxRyFBMG5ID3EDqgFpC4DeKismwCuVwutnwqoWATMgbhhOHP59N6XP/tdtPr9bKLbumTKs3skoCofIhlyFCIZapT7P56XHLc8OwU/gIkHi9txZ0KrQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"managed\"\n        title=\"\"\n        src=\"/static/b31cce763de8128348beb4a33a705cff/5a190/managed.png\"\n        srcset=\"/static/b31cce763de8128348beb4a33a705cff/772e8/managed.png 200w,\n/static/b31cce763de8128348beb4a33a705cff/e17e5/managed.png 400w,\n/static/b31cce763de8128348beb4a33a705cff/5a190/managed.png 800w,\n/static/b31cce763de8128348beb4a33a705cff/c1b63/managed.png 1200w,\n/static/b31cce763de8128348beb4a33a705cff/07d7d/managed.png 1478w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>When <code class=\"language-text\">READY</code> and <code class=\"language-text\">SYNCED</code> for all managed resources show value 'True' this means that everything was provisioned successfully in AWS. If you see it stuck in False we can run <code class=\"language-text\">kubectl describe {replace with managed resource}</code> to see more details about why its stuck.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c001eba8adbd7c9293c346acb77424f4/2dab4/managed1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAABrklEQVR42n2TS3OCQBCE8YkgKgqKCIKPWB58VE45pFLJIf//N3X4tjKpVDQeurZ2YXt6ume93W6rqlorWrxpWnwoyV81K96UlZ/Kn19UvRdKskRpkioIA3me9xi+7ysIAkXDvsKg5xD4XQ2HvsajSHEUqywKzWYzjUYj9ft9d7HVajncEB4OB9V1rXqz1abB/unQKK5VlpWKstRyudR0OlWWZQ7X61XH41Hr9bopOrwlTJLEXTCkaao4jp2i+XyuxWKh4lthFEUOKGXtdDq3hFyGiAvAiCk0Ho8dKJLnueuEc4C6uy1TDXDRFEwmE7dfrVZOIWd4jSL8Bt1u934oVC4br2gL4BmAyCzgn9Pp1Hi8cer+JQP4ZIYDiDiDBDsg4Jw9oNV2u/0/oQXw20Nr37wkUbrg7KG634QWjKmiVYDi8nt82OP3Q4W0gCILx4KxtC3ZMAzvz91fQMZFlNkIcWZ7m0cD33ktvV7PrWAwGLjk3Vzu93vn0XbLm65+VhIldVPNKNE+r4Rv/McrO5/Pulwu7l9GyzPvqG7BQIA6lJIs/kFor4Rg8BGgygac9QvyZEaVRDeHbQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"managed\"\n        title=\"\"\n        src=\"/static/c001eba8adbd7c9293c346acb77424f4/5a190/managed1.png\"\n        srcset=\"/static/c001eba8adbd7c9293c346acb77424f4/772e8/managed1.png 200w,\n/static/c001eba8adbd7c9293c346acb77424f4/e17e5/managed1.png 400w,\n/static/c001eba8adbd7c9293c346acb77424f4/5a190/managed1.png 800w,\n/static/c001eba8adbd7c9293c346acb77424f4/c1b63/managed1.png 1200w,\n/static/c001eba8adbd7c9293c346acb77424f4/2dab4/managed1.png 1461w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Additionally you can login to AWS console and verify all resources are successfully provisioned. Screen capture below shows EKS cluster provisioned with default managed node group</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fda82e59ef9ce5fb4d21224917c7a980/cc6fe/crossplanecluster.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNElEQVR42mVRB27DMBDz/38RFAmaZbfp+lNaeMinLZm9k53VGiCOoqhbrrbNCc3pC+ttjdV6h832iEPzjv0fPD0fsdrssavfHvSa3x5ePgo/vn6i8jFimia0o8V5ILRkQT5BhwS6h0+Lnq98lJgm2DwVPmiLahwJkZP2SkE7B+cDLEdjLBRpGOdh7Kz3pBgDaw6ez6LLvQ+BEUusApOcM9quxXfb4aftoYYBigv0HAeOwklrBi3Q0OX8HxX4k5GJu7FcMcaAwJUcd5FSKsUE0rV0kHiaGFPxyL1AeF74LSFX9t4vD2IxXaLosgJjTCl00e89TjzWzgk5I7qux5nHLfsqu/TcseXRzNU80lzULTvWXMC72ado5Cnp1qGYiU0xZUZ6gIwqCSVJkAkW/cqj/BBf1vIL9WRe/LbYGnsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"crossplanecluster\"\n        title=\"\"\n        src=\"/static/fda82e59ef9ce5fb4d21224917c7a980/5a190/crossplanecluster.png\"\n        srcset=\"/static/fda82e59ef9ce5fb4d21224917c7a980/772e8/crossplanecluster.png 200w,\n/static/fda82e59ef9ce5fb4d21224917c7a980/e17e5/crossplanecluster.png 400w,\n/static/fda82e59ef9ce5fb4d21224917c7a980/5a190/crossplanecluster.png 800w,\n/static/fda82e59ef9ce5fb4d21224917c7a980/c1b63/crossplanecluster.png 1200w,\n/static/fda82e59ef9ce5fb4d21224917c7a980/29007/crossplanecluster.png 1600w,\n/static/fda82e59ef9ce5fb4d21224917c7a980/cc6fe/crossplanecluster.png 1860w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Browse to VPC in console and verify all resource provisioned. Screen capture below shows VPC that was created by crossplane</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3229cd7e74c98f5b60eb53e65c1a7057/20785/vpc.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA6UlEQVR42q1RXUvEMBDMD/bl4K7qWfVA7oT7o4pc03w2adNm3E2kCPogamCY3dnJEDbi4fmM/eGMTdPianOL7c2B8Ijr9glNe8J2f8SOeHd3XMF6c38qemHWPljEGJHzgpQSxmkCnzTPmDNzwrLUWc651HW+VA/1E/FCszHNxSOkMrDOYwgDOinRK4VLr/FGkFQroyB7BWMttGGvQ0ez1458SuPl0kNqU7gnFhwWQkSgl/IFvsiwBW7tP8M6C7fW1eNc7QWHeD98CfwtREn3/n8DQwhr4F8h+BM4LNIeNS01xvFH4L1/p78DzQJNrHPBLW4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"vpc\"\n        title=\"\"\n        src=\"/static/3229cd7e74c98f5b60eb53e65c1a7057/5a190/vpc.png\"\n        srcset=\"/static/3229cd7e74c98f5b60eb53e65c1a7057/772e8/vpc.png 200w,\n/static/3229cd7e74c98f5b60eb53e65c1a7057/e17e5/vpc.png 400w,\n/static/3229cd7e74c98f5b60eb53e65c1a7057/5a190/vpc.png 800w,\n/static/3229cd7e74c98f5b60eb53e65c1a7057/c1b63/vpc.png 1200w,\n/static/3229cd7e74c98f5b60eb53e65c1a7057/20785/vpc.png 1307w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>So far I'm really liking what I've been able to do with Crossplane, I plan to extend this to be able to define cluster recipes (Small/Medium, Large as an example) so folks requesting cluster don't even have to know anything about networking, compute etc. Also I could have defined XRDs more provider agnostic and not use <code class=\"language-text\">EKS</code>, <code class=\"language-text\">Availability Zone</code> etc in the api spec.</p>\n<p>Hope this post helped spawn new ideas if you are a platform builder or building tools for enterprise.</p>\n<p>As always feel free to reach out if you have any questions about this post</p>\n<p>Thanks,\nRam</p>","id":"e635bc93-9143-5a2b-85af-e6b631ccdc29","frontmatter":{"title":"Automating provisioning of EKS Clusters with Crossplane","date":"January 30, 2024","tags":["Crossplane","AWS","EKS","Tools","Kubernetes"],"author":"Ram Gopinathan"},"fields":{"slug":"/provisioning-eks-clusters-with-crossplane/"}}}]}},"pageContext":{"limit":3,"skip":9,"numPages":15,"currentPage":4}},"staticQueryHashes":["1611934721","2366241629"],"slicesMap":{}}